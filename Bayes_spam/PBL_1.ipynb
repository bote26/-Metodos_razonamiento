{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba36f7a",
   "metadata": {},
   "source": [
    "# Actividad PBL 1\n",
    "\n",
    "### César Isao Pastelin Kohagura - A01659947\n",
    "\n",
    "### Sophia Gabriela Martínez Albarrán - A01424430\n",
    "\n",
    "### Luis Emilio Fernández González - A01659517\n",
    "\n",
    "### Eduardo Botello Casey - A01659281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd219333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc80e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sophiagabrielamartinezalbarran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sophiagabrielamartinezalbarran/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fc6d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data.drop([data.columns[col] for col in [2, 3, 4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfd0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos los datos, los tokenizamos y los regresamos como una lista de palabras\n",
    "def procesarEmail(contents):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    contenido = contents.lower()\n",
    "    contenido = re.sub(r'<[^<>]+>', ' ', contenido)\n",
    "    contenido = re.sub(r'[0-9]+', 'number', contenido)\n",
    "    contenido = re.sub(r'(http|https)://[^\\s]*', 'httpaddr', contenido)\n",
    "    contenido = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', contenido)\n",
    "    contenido = re.sub(r'[$]+', 'dollar', contenido)\n",
    "\n",
    "    palabras = word_tokenize(contenido)\n",
    "\n",
    "    for i in range(len(palabras)):\n",
    "        palabras[i] = re.sub(r'[^a-zA-Z0-9]', '', palabras[i])\n",
    "        palabras[i] = ps.stem(palabras[i])\n",
    "\n",
    "    palabras = [word for word in palabras if len(word) >= 1]\n",
    "\n",
    "    return palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e239e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibimos la lista de las palabras usadas en todos los emails \n",
    "# y creamos un vocabulario con las palabras más frecuentes\n",
    "def getVocabulario(emails, longitud_vocabulario):\n",
    "    vocabulario = dict()\n",
    "    \n",
    "    for i in range(len(emails)):\n",
    "        emails[i] = procesarEmail(emails[i])\n",
    "        for palabra in emails[i]:\n",
    "            if palabra in vocabulario.keys():\n",
    "                vocabulario[palabra] += 1\n",
    "            else:\n",
    "                vocabulario[palabra] = 1\n",
    "\n",
    "    vocabulario = sorted(vocabulario.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocabulario = list(map(lambda x: x[0], vocabulario[0:longitud_vocabulario]))\n",
    "    vocabulario = {index: word for index, word in enumerate(vocabulario)}\n",
    "\n",
    "    return vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab7203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrimos el diccionario buscando la clave de un valor específico\n",
    "def getllave(dictionario, val):\n",
    "    for key, value in dictionario.items():\n",
    "        if value == val:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f07f693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devolvemos los índices de las palabras del vocabulario que aparecen en un email\n",
    "def getIndices(email, vocabulario):\n",
    "    indice_palabras = set()\n",
    "    \n",
    "    for palabra in email:\n",
    "        if palabra in vocabulario.values():\n",
    "            indice_palabras.add(getllave(vocabulario, palabra))\n",
    "\n",
    "    return indice_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85e27d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización del email para el modelo\n",
    "def obtenerVectorCaracteristico(indice_palabras, longitud_vocabulario):\n",
    "    VectorCaracteristico = np.zeros(longitud_vocabulario)\n",
    "\n",
    "    for i in indice_palabras:\n",
    "        VectorCaracteristico[i] = 1\n",
    "\n",
    "    return VectorCaracteristico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0ec25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_vocabulario= 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6b3072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construímos el vocabulario con las palabras más frecuentes\n",
    "vocabulario = getVocabulario(data['v2'].to_list(), longitud_vocabulario)\n",
    "\n",
    "# Limpiamos y tokenizamos todos los correos\n",
    "emails = data['v2'].to_list()\n",
    "emails = list(map(lambda x: procesarEmail(x), emails))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5b20d",
   "metadata": {},
   "source": [
    "### Implementacion del Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17ca0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.probabilidad_clase = dict()\n",
    "        self.probabilidad_palabra = dict()\n",
    "        self.clases = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Obtenemos las clases únicas, en este caso spam y ham\n",
    "        self.clases = np.unique(y)\n",
    "        \n",
    "        # Guardamos la probabilidad de cada clase y la probabilidad de cada palabra dado una clase en un diccionario para luego poder hacer las predicciones\n",
    "        for c in self.clases:\n",
    "            X_c = X[y == c]\n",
    "            self.probabilidad_clase[c] = X_c.shape[0] / X.shape[0]\n",
    "            \n",
    "            # Suavizado de Laplace: Se usa para evitar probabilidades de cero, ya que si predecimos un nuevo email que no contiene una palabra del vocabulario \n",
    "            # la probabilidad de esa palabra sería cero, lo cual afectaría el cálculo de la probabilidad total por lo que asumimos que todas las palabras\n",
    "            # al menos aparecen una vez en cada clase\n",
    "            self.probabilidad_palabra[c] = (X_c.sum(axis=0) + 1) / (X_c.shape[0] + 2)\n",
    "    \n",
    "    def predecir(self, X):\n",
    "        predicciones = []\n",
    "        \n",
    "        # Predecimos la clase para cada email\n",
    "        for i in range(X.shape[0]):\n",
    "            prediciones_por_clase = dict()\n",
    "            \n",
    "            # Probabilidad a priori de cada clase\n",
    "            for c in self.clases:\n",
    "                # Vamos a usar logaritmos para evitar underflow y para que las probabilidades sean más manejables, tambien para evitar\n",
    "                # multiplicar y mejor sumar, log(a x b) = log(a) + log(b)\n",
    "                probabilidad_clase_c = np.log(self.probabilidad_clase[c])\n",
    "                \n",
    "                # Tomamos en cuenta la probabilidad de cada palabra en el email dado la clase, pero también la probabilidad de que no aparezca\n",
    "                # ya que ambas cosas nos dan información sobre la clase del email\n",
    "                probabilidad = X.iloc[i] * np.log(self.probabilidad_palabra[c]) + (1 - X.iloc[i]) * np.log(1 - self.probabilidad_palabra[c])\n",
    "                probabilidad = probabilidad.sum()\n",
    "                probabilidad_final = probabilidad_clase_c + probabilidad\n",
    "                prediciones_por_clase[c] = probabilidad_final\n",
    "            \n",
    "            # Tomamos la clase con la probabilidad más alta, es decir si P(clase|email) es mayor para spam que para ham, entonces el email es spam\n",
    "            predicciones.append(max(prediciones_por_clase, key=prediciones_por_clase.get))\n",
    "        \n",
    "        return np.array(predicciones)\n",
    "\n",
    "    def predecir_proba(self, X):\n",
    "        probabilidades = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Calculamos P(email|clase) × P(clase) para cada clase\n",
    "            numeradores = dict()\n",
    "            \n",
    "            for c in self.clases:\n",
    "                # P(clase) - probabilidad a priori\n",
    "                #Es la proporción de correos spam o ham en el dataset\n",
    "                priori = self.probabilidad_clase[c]\n",
    "                \n",
    "                # P(email|clase) - probabilidad\n",
    "                # Si la palabra aparece en el email entonces se va a multiplicar P(palabra|clase)\n",
    "                # Si la palabra no aparece en el email se multiplica por (1 - P(palabra|clase))\n",
    "                probabilidad = 1.0\n",
    "                for j in range(len(X.iloc[i])):\n",
    "                    if X.iloc[i].iloc[j] == 1:  # palabra presente\n",
    "                        probabilidad *= self.probabilidad_palabra[c].iloc[j]\n",
    "                    else:  # palabra ausente\n",
    "                        probabilidad *= (1 - self.probabilidad_palabra[c].iloc[j])\n",
    "                \n",
    "                # P(email|clase) × P(clase)\n",
    "                #Es la probabilidad conjunta de que el email y la clase ocurran juntos.\n",
    "                numeradores[c] = probabilidad * priori\n",
    "\n",
    "            # P(email) = suma de todos los numeradores\n",
    "            #Es la probabilidad total del email sin importar la clase.\n",
    "            #Se necesita para normalizar y que las probabilidades finales sumen 1.\n",
    "            # Sale de la ley de probabilidad total\n",
    "            # P(email) = P(email|ham)×P(ham) + P(email|spam)×P(spam)\n",
    "            denominador = sum(numeradores.values())\n",
    "            \n",
    "            # P(clase|email) = P(email|clase) × P(clase) / P(email)\n",
    "            # Esto nos da la probabilidad de que el email pertenezca a cada clase (Spam o Ham)\n",
    "            probabilidades_reales = [numeradores[c] / denominador for c in sorted(self.clases)]\n",
    "            probabilidades.append(probabilidades_reales)\n",
    "            \n",
    "        return np.array(probabilidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8987572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(map(lambda x: obtenerVectorCaracteristico(getIndices(x, vocabulario), longitud_vocabulario), emails))\n",
    "X = pd.DataFrame(np.array(X).astype(np.int16))\n",
    "y = data['v1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8b907",
   "metadata": {},
   "source": [
    "## Entrenamiento y prueba del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo = NaiveBayes()\n",
    "modelo.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "308a859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÉTRICAS DE EVALUACIÓN ===\n",
      "Accuracy : 0.9812\n",
      "Precision : 0.9640\n",
      "Recall : 0.8933\n",
      "F1-Score : 0.9273\n",
      "\n",
      "=== MATRIZ DE CONFUSIÓN ===\n",
      "                Predicho\n",
      "              Ham    Spam\n",
      "Real   Ham     960       5\n",
      "       Spam     16     134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Predicciones\n",
    "y_pred = modelo.predecir(X_test)\n",
    "y_proba = modelo.predecir_proba(X_test)\n",
    "\n",
    "# Métricas básicas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "\n",
    "print(\"=== MÉTRICAS DE EVALUACIÓN ===\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['ham', 'spam'])\n",
    "\n",
    "print(\"\\n=== MATRIZ DE CONFUSIÓN ===\")\n",
    "print(\"                Predicho\")\n",
    "print(\"              Ham    Spam\")\n",
    "print(f\"Real   Ham    {cm[0,0]:4d}    {cm[0,1]:4d}\")\n",
    "print(f\"       Spam   {cm[1,0]:4d}    {cm[1,1]:4d}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

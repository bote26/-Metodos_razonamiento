{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ff1006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fc6d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e28bcf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dfe2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([data.columns[col] for col in [2, 3, 4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos los datos, los tokenizamos y los regresamos como una lista de palabras\n",
    "def processEmail(contents):\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    contents = contents.lower()\n",
    "    contents = re.sub(r'<[^<>]+>', ' ', contents)\n",
    "    contents = re.sub(r'[0-9]+', 'number', contents)\n",
    "    contents = re.sub(r'(http|https)://[^\\s]*', 'httpaddr', contents)\n",
    "    contents = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', contents)\n",
    "    contents = re.sub(r'[$]+', 'dollar', contents)\n",
    "    \n",
    "    words = word_tokenize(contents)\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        words[i] = re.sub(r'[^a-zA-Z0-9]', '', words[i])\n",
    "        words[i] = ps.stem(words[i])\n",
    "        \n",
    "    words = [word for word in words if len(word) >= 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53e239e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibimos la lista de las palabras usadas en todos los emails \n",
    "# y creamos un vocabulario con las palabras más frecuentes\n",
    "def getVocabulary(emails, vocab_length):\n",
    "    vocabulary = dict()\n",
    "    \n",
    "    for i in range(len(emails)):\n",
    "        emails[i] = processEmail(emails[i])\n",
    "        for word in emails[i]:\n",
    "            if word in vocabulary.keys():\n",
    "                vocabulary[word] += 1\n",
    "            else:\n",
    "                vocabulary[word] = 1\n",
    "                \n",
    "    vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocabulary = list(map(lambda x: x[0], vocabulary[0:vocab_length]))\n",
    "    vocabulary = {index: word for index, word in enumerate(vocabulary)}\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab7203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrimos el diccionario buscando la clave de un valor específico\n",
    "# Esto es para el modelo de Machine Learning\n",
    "def getKey(dictionary, val):\n",
    "    for key, value in dictionary.items():\n",
    "        if value == val:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f07f693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devolvemos los índices de las palabras del vocabulario que aparecen en un email\n",
    "# Esto es para el modelo de Machine Learning\n",
    "def getIndices(email, vocabulary):\n",
    "    word_indices = set()\n",
    "    \n",
    "    for word in email:\n",
    "        if word in vocabulary.values():\n",
    "            word_indices.add(getKey(vocabulary, word))\n",
    "    \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85e27d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es para el modelo de Machine Learning\n",
    "def getFeatureVector(word_indices, vocab_length):\n",
    "    feature_vec = np.zeros(vocab_length)\n",
    "    \n",
    "    for i in word_indices:\n",
    "        feature_vec[i] = 1\n",
    "        \n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0ec25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb5fa7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sophiagabrielamartinezalbarran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sophiagabrielamartinezalbarran/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6b3072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construímos el vocabulario con las palabras más frecuentes\n",
    "vocabulary = getVocabulary(data['v2'].to_list(), vocab_length)\n",
    "\n",
    "# Limpiamos y tokenizamos todos los correos\n",
    "emails = data['v2'].to_list()\n",
    "emails = list(map(lambda x: processEmail(x), emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67202b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'i',\n",
       " 1: 'number',\n",
       " 2: 'to',\n",
       " 3: 'you',\n",
       " 4: 'a',\n",
       " 5: 'the',\n",
       " 6: 'u',\n",
       " 7: 'and',\n",
       " 8: 'it',\n",
       " 9: 'is',\n",
       " 10: 'in',\n",
       " 11: 'me',\n",
       " 12: 'my',\n",
       " 13: 'for',\n",
       " 14: 'your',\n",
       " 15: 'call',\n",
       " 16: 'have',\n",
       " 17: 'do',\n",
       " 18: 'that',\n",
       " 19: 'of',\n",
       " 20: 's',\n",
       " 21: 'on',\n",
       " 22: 'are',\n",
       " 23: 'now',\n",
       " 24: 'so',\n",
       " 25: 'go',\n",
       " 26: 'get',\n",
       " 27: 'not',\n",
       " 28: 'but',\n",
       " 29: 'be',\n",
       " 30: 'or',\n",
       " 31: 'm',\n",
       " 32: 'can',\n",
       " 33: 'at',\n",
       " 34: 'we',\n",
       " 35: 'will',\n",
       " 36: 'if',\n",
       " 37: 'ur',\n",
       " 38: 'with',\n",
       " 39: 'nt',\n",
       " 40: 'just',\n",
       " 41: 'no',\n",
       " 42: 'thi',\n",
       " 43: 'how',\n",
       " 44: 'gt',\n",
       " 45: 'lt',\n",
       " 46: 'up',\n",
       " 47: 'what',\n",
       " 48: 'come',\n",
       " 49: 'when',\n",
       " 50: 'ok',\n",
       " 51: 'from',\n",
       " 52: 'free',\n",
       " 53: 'know',\n",
       " 54: 'all',\n",
       " 55: 'out',\n",
       " 56: 'like',\n",
       " 57: 'got',\n",
       " 58: 'love',\n",
       " 59: 'day',\n",
       " 60: 'time',\n",
       " 61: 'wa',\n",
       " 62: 'want',\n",
       " 63: 'good',\n",
       " 64: 'then',\n",
       " 65: 'll',\n",
       " 66: 'there',\n",
       " 67: 'he',\n",
       " 68: 'text',\n",
       " 69: 'am',\n",
       " 70: 'onli',\n",
       " 71: 'send',\n",
       " 72: 'hi',\n",
       " 73: 'need',\n",
       " 74: 'one',\n",
       " 75: 'txt',\n",
       " 76: 'as',\n",
       " 77: 'today',\n",
       " 78: 'see',\n",
       " 79: 'by',\n",
       " 80: 'take',\n",
       " 81: 'think',\n",
       " 82: 'about',\n",
       " 83: 'she',\n",
       " 84: 'did',\n",
       " 85: 'home',\n",
       " 86: 'repli',\n",
       " 87: 'lor',\n",
       " 88: 'r',\n",
       " 89: 'stop',\n",
       " 90: 'sorri',\n",
       " 91: 'tell',\n",
       " 92: 'still',\n",
       " 93: 'back',\n",
       " 94: 'dont',\n",
       " 95: 'mobil',\n",
       " 96: 'da',\n",
       " 97: 'our',\n",
       " 98: 'n',\n",
       " 99: 'make',\n",
       " 100: 'phone',\n",
       " 101: 'say',\n",
       " 102: 'ha',\n",
       " 103: 'pleas',\n",
       " 104: 'd',\n",
       " 105: 'new',\n",
       " 106: 'work',\n",
       " 107: 'later',\n",
       " 108: 'week',\n",
       " 109: 'ani',\n",
       " 110: 'they',\n",
       " 111: 'ask',\n",
       " 112: 'been',\n",
       " 113: 'miss',\n",
       " 114: 'hope',\n",
       " 115: 'pl',\n",
       " 116: 'an',\n",
       " 117: 'her',\n",
       " 118: 'meet',\n",
       " 119: 'k',\n",
       " 120: 'messag',\n",
       " 121: 'who',\n",
       " 122: 'here',\n",
       " 123: 'some',\n",
       " 124: 'happi',\n",
       " 125: 'night',\n",
       " 126: 'wait',\n",
       " 127: 'well',\n",
       " 128: 'dear',\n",
       " 129: 'where',\n",
       " 130: 'great',\n",
       " 131: 'claim',\n",
       " 132: 'thing',\n",
       " 133: 'oh',\n",
       " 134: 'tri',\n",
       " 135: 'much',\n",
       " 136: 'wat',\n",
       " 137: 'give',\n",
       " 138: 'c',\n",
       " 139: 'hey',\n",
       " 140: 'him',\n",
       " 141: 'more',\n",
       " 142: 're',\n",
       " 143: 'na',\n",
       " 144: 'too',\n",
       " 145: 'friend',\n",
       " 146: 'had',\n",
       " 147: 'thank',\n",
       " 148: 'way',\n",
       " 149: 'should',\n",
       " 150: 'msg',\n",
       " 151: 've',\n",
       " 152: 'prize',\n",
       " 153: 'right',\n",
       " 154: 'ye',\n",
       " 155: 'feel',\n",
       " 156: 'let',\n",
       " 157: 'im',\n",
       " 158: 'wan',\n",
       " 159: 'even',\n",
       " 160: 'alreadi',\n",
       " 161: 'pick',\n",
       " 162: 'tomorrow',\n",
       " 163: 'after',\n",
       " 164: 'said',\n",
       " 165: 'yeah',\n",
       " 166: 'min',\n",
       " 167: 'realli',\n",
       " 168: 'amp',\n",
       " 169: 'leav',\n",
       " 170: 'e',\n",
       " 171: 'babe',\n",
       " 172: 'care',\n",
       " 173: 'co',\n",
       " 174: 'whi',\n",
       " 175: 'them',\n",
       " 176: 'morn',\n",
       " 177: 'would',\n",
       " 178: 'veri',\n",
       " 179: 'win',\n",
       " 180: 'life',\n",
       " 181: 'last',\n",
       " 182: 'sleep',\n",
       " 183: 'numbernumb',\n",
       " 184: 'sure',\n",
       " 185: 'servic',\n",
       " 186: 'keep',\n",
       " 187: 'use',\n",
       " 188: 'ill',\n",
       " 189: 'cash',\n",
       " 190: 'find',\n",
       " 191: 'year',\n",
       " 192: 'contact',\n",
       " 193: 't',\n",
       " 194: 'lol',\n",
       " 195: 'anyth',\n",
       " 196: 'buy',\n",
       " 197: 'tone',\n",
       " 198: 'won',\n",
       " 199: 'look',\n",
       " 200: 'wish',\n",
       " 201: 'everi',\n",
       " 202: 'nokia',\n",
       " 203: 'start',\n",
       " 204: 'smile',\n",
       " 205: 'also',\n",
       " 206: 'sent',\n",
       " 207: 'numberp',\n",
       " 208: 'urgent',\n",
       " 209: 'watch',\n",
       " 210: 'someth',\n",
       " 211: 'show',\n",
       " 212: 'befor',\n",
       " 213: 'over',\n",
       " 214: 'cant',\n",
       " 215: 'b',\n",
       " 216: 'finish',\n",
       " 217: 'end',\n",
       " 218: 'award',\n",
       " 219: 'again',\n",
       " 220: 'were',\n",
       " 221: 'could',\n",
       " 222: 'place',\n",
       " 223: 'us',\n",
       " 224: 'other',\n",
       " 225: 'first',\n",
       " 226: 'custom',\n",
       " 227: 'next',\n",
       " 228: 'someon',\n",
       " 229: 'guy',\n",
       " 230: 'around',\n",
       " 231: 'talk',\n",
       " 232: 'tonight',\n",
       " 233: 'help',\n",
       " 234: 'which',\n",
       " 235: 'late',\n",
       " 236: 'went',\n",
       " 237: 'word',\n",
       " 238: 'ca',\n",
       " 239: 'collect',\n",
       " 240: 'chat',\n",
       " 241: 'off',\n",
       " 242: 'gon',\n",
       " 243: 'soon',\n",
       " 244: 'money',\n",
       " 245: 'person',\n",
       " 246: 'mani',\n",
       " 247: 'per',\n",
       " 248: 'nice',\n",
       " 249: 'plan',\n",
       " 250: 'ya',\n",
       " 251: 'live',\n",
       " 252: 'down',\n",
       " 253: 'alway',\n",
       " 254: 'minut',\n",
       " 255: 'dun',\n",
       " 256: 'check',\n",
       " 257: 'gud',\n",
       " 258: 'name',\n",
       " 259: 'lot',\n",
       " 260: 'told',\n",
       " 261: 'special',\n",
       " 262: 'mean',\n",
       " 263: 'v',\n",
       " 264: 'girl',\n",
       " 265: 'heart',\n",
       " 266: 'hour',\n",
       " 267: 'x',\n",
       " 268: 'hello',\n",
       " 269: 'reach',\n",
       " 270: 'peopl',\n",
       " 271: 'haha',\n",
       " 272: 'shop',\n",
       " 273: 'fine',\n",
       " 274: 'same',\n",
       " 275: 'guarante',\n",
       " 276: 'yet',\n",
       " 277: 'happen',\n",
       " 278: 'thk',\n",
       " 279: 'may',\n",
       " 280: 'done',\n",
       " 281: 'offer',\n",
       " 282: 'thought',\n",
       " 283: 'play',\n",
       " 284: 'doe',\n",
       " 285: 'class',\n",
       " 286: 'holiday',\n",
       " 287: 'line',\n",
       " 288: 'fuck',\n",
       " 289: 'receiv',\n",
       " 290: 'lunch',\n",
       " 291: 'best',\n",
       " 292: 'god',\n",
       " 293: 'stuff',\n",
       " 294: 'eat',\n",
       " 295: 'car',\n",
       " 296: 'man',\n",
       " 297: 'job',\n",
       " 298: 'draw',\n",
       " 299: 'mayb',\n",
       " 300: 'bit',\n",
       " 301: 'never',\n",
       " 302: 'y',\n",
       " 303: 'few',\n",
       " 304: 'enjoy',\n",
       " 305: 'month',\n",
       " 306: 'worri',\n",
       " 307: 'yup',\n",
       " 308: 'sm',\n",
       " 309: 'account',\n",
       " 310: 'long',\n",
       " 311: 'drive',\n",
       " 312: 'guess',\n",
       " 313: 'better',\n",
       " 314: 'dat',\n",
       " 315: 'readi',\n",
       " 316: 'problem',\n",
       " 317: 'mind',\n",
       " 318: 'numberst',\n",
       " 319: 'chanc',\n",
       " 320: 'date',\n",
       " 321: 'cool',\n",
       " 322: 'cs',\n",
       " 323: 'world',\n",
       " 324: 'latest',\n",
       " 325: 'cost',\n",
       " 326: 'pay',\n",
       " 327: 'weekend',\n",
       " 328: 'wonder',\n",
       " 329: 'room',\n",
       " 330: 'numberppm',\n",
       " 331: 'numbernumbernumb',\n",
       " 332: 'sir',\n",
       " 333: 'boy',\n",
       " 334: 'becaus',\n",
       " 335: 'than',\n",
       " 336: 'didnt',\n",
       " 337: 'bring',\n",
       " 338: 'quit',\n",
       " 339: 'lar',\n",
       " 340: 'half',\n",
       " 341: 'yo',\n",
       " 342: 'noth',\n",
       " 343: 'hous',\n",
       " 344: 'book',\n",
       " 345: 'game',\n",
       " 346: 'sweet',\n",
       " 347: 'anoth',\n",
       " 348: 'luv',\n",
       " 349: 'liao',\n",
       " 350: 'big',\n",
       " 351: 'voucher',\n",
       " 352: 'question',\n",
       " 353: 'select',\n",
       " 354: 'camera',\n",
       " 355: 'charg',\n",
       " 356: 'real',\n",
       " 357: 'birthday',\n",
       " 358: 'landlin',\n",
       " 359: 'stay',\n",
       " 360: 'into',\n",
       " 361: 'shit',\n",
       " 362: 'kiss',\n",
       " 363: 'put',\n",
       " 364: 'speak',\n",
       " 365: 'rememb',\n",
       " 366: 'dinner',\n",
       " 367: 'rington',\n",
       " 368: 'join',\n",
       " 369: 'ju',\n",
       " 370: 'box',\n",
       " 371: 'ever',\n",
       " 372: 'pic',\n",
       " 373: 'rate',\n",
       " 374: 'ah',\n",
       " 375: 'might',\n",
       " 376: 'actual',\n",
       " 377: 'point',\n",
       " 378: 'final',\n",
       " 379: 'appli',\n",
       " 380: 'earli',\n",
       " 381: 'network',\n",
       " 382: 'di',\n",
       " 383: 'hear',\n",
       " 384: 'onc',\n",
       " 385: 'chang',\n",
       " 386: 'aight',\n",
       " 387: 'babi',\n",
       " 388: 'probabl',\n",
       " 389: 'fun',\n",
       " 390: 'xxx',\n",
       " 391: 'wont',\n",
       " 392: 'run',\n",
       " 393: 'part',\n",
       " 394: 'pa',\n",
       " 395: 'bed',\n",
       " 396: 'hurt',\n",
       " 397: 'numberth',\n",
       " 398: 'anyway',\n",
       " 399: 'answer',\n",
       " 400: 'po',\n",
       " 401: 'video',\n",
       " 402: 'two',\n",
       " 403: 'orang',\n",
       " 404: 'den',\n",
       " 405: 'bad',\n",
       " 406: 'princess',\n",
       " 407: 'code',\n",
       " 408: 'between',\n",
       " 409: 'forgot',\n",
       " 410: 'easi',\n",
       " 411: 'thanx',\n",
       " 412: 'wake',\n",
       " 413: 'shall',\n",
       " 414: 'dunno',\n",
       " 415: 'sat',\n",
       " 416: 'offic',\n",
       " 417: 'left',\n",
       " 418: 'numbernd',\n",
       " 419: 'frnd',\n",
       " 420: 'littl',\n",
       " 421: 'dream',\n",
       " 422: 'tv',\n",
       " 423: 'leh',\n",
       " 424: 'walk',\n",
       " 425: 'face',\n",
       " 426: 'dad',\n",
       " 427: 'enough',\n",
       " 428: 'bu',\n",
       " 429: 'pain',\n",
       " 430: 'afternoon',\n",
       " 431: 'movi',\n",
       " 432: 'numberhr',\n",
       " 433: 'school',\n",
       " 434: 'those',\n",
       " 435: 'sound',\n",
       " 436: 'everyth',\n",
       " 437: 'made',\n",
       " 438: 'detail',\n",
       " 439: 'mate',\n",
       " 440: 'pound',\n",
       " 441: 'most',\n",
       " 442: 'mail',\n",
       " 443: 'without',\n",
       " 444: 'xma',\n",
       " 445: 'tmr',\n",
       " 446: 'lose',\n",
       " 447: 'read',\n",
       " 448: 'post',\n",
       " 449: 'bore',\n",
       " 450: 'while',\n",
       " 451: 'gift',\n",
       " 452: 'await',\n",
       " 453: 'until',\n",
       " 454: 'wif',\n",
       " 455: 'though',\n",
       " 456: 'credit',\n",
       " 457: 'decid',\n",
       " 458: 'sinc',\n",
       " 459: 'came',\n",
       " 460: 'test',\n",
       " 461: 'must',\n",
       " 462: 'sexi',\n",
       " 463: 'hav',\n",
       " 464: 'town',\n",
       " 465: 'entri',\n",
       " 466: 'goe',\n",
       " 467: 'set',\n",
       " 468: 'colour',\n",
       " 469: 'uk',\n",
       " 470: 'lesson',\n",
       " 471: 'close',\n",
       " 472: 'havent',\n",
       " 473: 'abt',\n",
       " 474: 'wk',\n",
       " 475: 'okay',\n",
       " 476: 'price',\n",
       " 477: 'til',\n",
       " 478: 'abl',\n",
       " 479: 'import',\n",
       " 480: 'true',\n",
       " 481: 'updat',\n",
       " 482: 'mob',\n",
       " 483: 'juz',\n",
       " 484: 'enter',\n",
       " 485: 'order',\n",
       " 486: 'bath',\n",
       " 487: 'smoke',\n",
       " 488: 'decim',\n",
       " 489: 'plz',\n",
       " 490: 'wot',\n",
       " 491: 'poli',\n",
       " 492: 'drink',\n",
       " 493: 'away',\n",
       " 494: 'plu',\n",
       " 495: 'wife',\n",
       " 496: 'till',\n",
       " 497: 'wo',\n",
       " 498: 'saw',\n",
       " 499: 'yesterday',\n",
       " 500: 'hair',\n",
       " 501: 'wen',\n",
       " 502: 'els',\n",
       " 503: 'top',\n",
       " 504: 'bt',\n",
       " 505: 'music',\n",
       " 506: 'weekli',\n",
       " 507: 'dude',\n",
       " 508: 'beauti',\n",
       " 509: 'attempt',\n",
       " 510: 'de',\n",
       " 511: 'drop',\n",
       " 512: 'valid',\n",
       " 513: 'alright',\n",
       " 514: 'invit',\n",
       " 515: 'doubl',\n",
       " 516: 'trip',\n",
       " 517: 'food',\n",
       " 518: 'haf',\n",
       " 519: 'hand',\n",
       " 520: 'oso',\n",
       " 521: 'head',\n",
       " 522: 'friendship',\n",
       " 523: 'onlin',\n",
       " 524: 'lei',\n",
       " 525: 'search',\n",
       " 526: 'ard',\n",
       " 527: 'deliveri',\n",
       " 528: 'yourself',\n",
       " 529: 'busi',\n",
       " 530: 'numberpmin',\n",
       " 531: 'yr',\n",
       " 532: 'coz',\n",
       " 533: 'open',\n",
       " 534: 'si',\n",
       " 535: 'ring',\n",
       " 536: 'hot',\n",
       " 537: 'either',\n",
       " 538: 'these',\n",
       " 539: 'sch',\n",
       " 540: 'famili',\n",
       " 541: 'goin',\n",
       " 542: 'brother',\n",
       " 543: 'mom',\n",
       " 544: 'second',\n",
       " 545: 'bonu',\n",
       " 546: 'caus',\n",
       " 547: 'address',\n",
       " 548: 'inform',\n",
       " 549: 'player',\n",
       " 550: 'complet',\n",
       " 551: 'stori',\n",
       " 552: 'id',\n",
       " 553: 'nite',\n",
       " 554: 'g',\n",
       " 555: 'hold',\n",
       " 556: 'wid',\n",
       " 557: 'full',\n",
       " 558: 'tot',\n",
       " 559: 'sae',\n",
       " 560: 'togeth',\n",
       " 561: 'email',\n",
       " 562: 'sad',\n",
       " 563: 'forget',\n",
       " 564: 'old',\n",
       " 565: 'match',\n",
       " 566: 'emailaddr',\n",
       " 567: 'content',\n",
       " 568: 'believ',\n",
       " 569: 'studi',\n",
       " 570: 'touch',\n",
       " 571: 'both',\n",
       " 572: 'noe',\n",
       " 573: 'club',\n",
       " 574: 'oki',\n",
       " 575: 'numberu',\n",
       " 576: 'chikku',\n",
       " 577: 'reason',\n",
       " 578: 'huh',\n",
       " 579: 'eve',\n",
       " 580: 'land',\n",
       " 581: 'listen',\n",
       " 582: 'train',\n",
       " 583: 'murder',\n",
       " 584: 'treat',\n",
       " 585: 'httpaddr',\n",
       " 586: 'aft',\n",
       " 587: 'fri',\n",
       " 588: 'tomo',\n",
       " 589: 'congrat',\n",
       " 590: 'took',\n",
       " 591: 'privat',\n",
       " 592: 'expir',\n",
       " 593: 'dog',\n",
       " 594: 'age',\n",
       " 595: 'everyon',\n",
       " 596: 'parent',\n",
       " 597: 'grnumber',\n",
       " 598: 'awesom',\n",
       " 599: 'break',\n",
       " 600: 'die',\n",
       " 601: 'unsubscrib',\n",
       " 602: 'simpl',\n",
       " 603: 'mum',\n",
       " 604: 'rite',\n",
       " 605: 'caller',\n",
       " 606: 'news',\n",
       " 607: 'tho',\n",
       " 608: 'move',\n",
       " 609: 'ta',\n",
       " 610: 'download',\n",
       " 611: 'prob',\n",
       " 612: 'statement',\n",
       " 613: 'fanci',\n",
       " 614: 'compani',\n",
       " 615: 'wil',\n",
       " 616: 'reveal',\n",
       " 617: 'angri',\n",
       " 618: 'choos',\n",
       " 619: 'sort',\n",
       " 620: 'card',\n",
       " 621: 'sister',\n",
       " 622: 'valentin',\n",
       " 623: 'current',\n",
       " 624: 'gd',\n",
       " 625: 'bnumber',\n",
       " 626: 'mine',\n",
       " 627: 'neva',\n",
       " 628: 'pub',\n",
       " 629: 'laugh',\n",
       " 630: 'anyon',\n",
       " 631: 'auction',\n",
       " 632: 'avail',\n",
       " 633: 'joke',\n",
       " 634: 'valu',\n",
       " 635: 'seem',\n",
       " 636: 'numberpm',\n",
       " 637: 'lucki',\n",
       " 638: 'bday',\n",
       " 639: 'type',\n",
       " 640: 'each',\n",
       " 641: 'bank',\n",
       " 642: 'worth',\n",
       " 643: 'found',\n",
       " 644: 'colleg',\n",
       " 645: 'park',\n",
       " 646: 'whatev',\n",
       " 647: 'knw',\n",
       " 648: 'sell',\n",
       " 649: 'understand',\n",
       " 650: 'alon',\n",
       " 651: 'winner',\n",
       " 652: 'pobox',\n",
       " 653: 'smth',\n",
       " 654: 'saturday',\n",
       " 655: 'usual',\n",
       " 656: 'pass',\n",
       " 657: 'song',\n",
       " 658: 'save',\n",
       " 659: 'oper',\n",
       " 660: 'ticket',\n",
       " 661: 'gone',\n",
       " 662: 'hit',\n",
       " 663: 'uncl',\n",
       " 664: 'unredeem',\n",
       " 665: 'identifi',\n",
       " 666: 'hard',\n",
       " 667: 'carlo',\n",
       " 668: 'log',\n",
       " 669: 'boytoy',\n",
       " 670: 'bill',\n",
       " 671: 'exam',\n",
       " 672: 'secret',\n",
       " 673: 'congratul',\n",
       " 674: 'anytim',\n",
       " 675: 'boxnumb',\n",
       " 676: 'far',\n",
       " 677: 'return',\n",
       " 678: 'numberpmsg',\n",
       " 679: 'mobileupdnumb',\n",
       " 680: 'welcom',\n",
       " 681: 'kind',\n",
       " 682: 'visit',\n",
       " 683: 'outsid',\n",
       " 684: 'o',\n",
       " 685: 'sun',\n",
       " 686: 'sit',\n",
       " 687: 'parti',\n",
       " 688: 'surpris',\n",
       " 689: 'crazi',\n",
       " 690: 'camcord',\n",
       " 691: 'cut',\n",
       " 692: 'follow',\n",
       " 693: 'xx',\n",
       " 694: 'rain',\n",
       " 695: 'friday',\n",
       " 696: 'mu',\n",
       " 697: 'their',\n",
       " 698: 'ltd',\n",
       " 699: 'nonumb',\n",
       " 700: 'wit',\n",
       " 701: 'darlin',\n",
       " 702: 'goodmorn',\n",
       " 703: 'oredi',\n",
       " 704: 'case',\n",
       " 705: 'tel',\n",
       " 706: 'fone',\n",
       " 707: 'light',\n",
       " 708: 'project',\n",
       " 709: 'bout',\n",
       " 710: 'th',\n",
       " 711: 'cum',\n",
       " 712: 'nope',\n",
       " 713: 'pretti',\n",
       " 714: 'sea',\n",
       " 715: 'fast',\n",
       " 716: 'clean',\n",
       " 717: 'drug',\n",
       " 718: 'gal',\n",
       " 719: 'wrong',\n",
       " 720: 'chennai',\n",
       " 721: 'tht',\n",
       " 722: 'wkli',\n",
       " 723: 'freemsg',\n",
       " 724: 'hungri',\n",
       " 725: 'confirm',\n",
       " 726: 'whole',\n",
       " 727: 'correct',\n",
       " 728: 'comput',\n",
       " 729: 'hmm',\n",
       " 730: 'spend',\n",
       " 731: 'dollar',\n",
       " 732: 'cours',\n",
       " 733: 'mrng',\n",
       " 734: 'ga',\n",
       " 735: 'meant',\n",
       " 736: 'fix',\n",
       " 737: 'cd',\n",
       " 738: 'unlimit',\n",
       " 739: 'own',\n",
       " 740: 'mnumber',\n",
       " 741: 'numbermth',\n",
       " 742: 'interest',\n",
       " 743: 'tc',\n",
       " 744: 'jay',\n",
       " 745: 'rock',\n",
       " 746: 'ad',\n",
       " 747: 'ten',\n",
       " 748: 'suppos',\n",
       " 749: 'differ',\n",
       " 750: 'scream',\n",
       " 751: 'remov',\n",
       " 752: 'term',\n",
       " 753: 'frm',\n",
       " 754: 'kid',\n",
       " 755: 'snow',\n",
       " 756: 'opt',\n",
       " 757: 'poboxnumb',\n",
       " 758: 'least',\n",
       " 759: 'press',\n",
       " 760: 'numberday',\n",
       " 761: 'info',\n",
       " 762: 'promis',\n",
       " 763: 'turn',\n",
       " 764: 'catch',\n",
       " 765: 'almost',\n",
       " 766: 'etc',\n",
       " 767: 'hee',\n",
       " 768: 'shower',\n",
       " 769: 'mah',\n",
       " 770: 'felt',\n",
       " 771: 'quiz',\n",
       " 772: 'tire',\n",
       " 773: 'ninumb',\n",
       " 774: 'wine',\n",
       " 775: 'joy',\n",
       " 776: 'mr',\n",
       " 777: 'doesnt',\n",
       " 778: 'march',\n",
       " 779: 'side',\n",
       " 780: 'fr',\n",
       " 781: 'dnt',\n",
       " 782: 'singl',\n",
       " 783: 'blue',\n",
       " 784: 'bslvyl',\n",
       " 785: 'lost',\n",
       " 786: 'christma',\n",
       " 787: 'figur',\n",
       " 788: 'moment',\n",
       " 789: 'st',\n",
       " 790: 'forward',\n",
       " 791: 'motorola',\n",
       " 792: 'coupl',\n",
       " 793: 'ass',\n",
       " 794: 'pm',\n",
       " 795: 'savamob',\n",
       " 796: 'sub',\n",
       " 797: 'within',\n",
       " 798: 'marri',\n",
       " 799: 'yar',\n",
       " 800: 'area',\n",
       " 801: 'paper',\n",
       " 802: 'sex',\n",
       " 803: 'earlier',\n",
       " 804: 'don',\n",
       " 805: 'support',\n",
       " 806: 'film',\n",
       " 807: 'fren',\n",
       " 808: 'w',\n",
       " 809: 'father',\n",
       " 810: 'reward',\n",
       " 811: 'eh',\n",
       " 812: 'nation',\n",
       " 813: 'eg',\n",
       " 814: 'cheer',\n",
       " 815: 'crave',\n",
       " 816: 'hospit',\n",
       " 817: 'wow',\n",
       " 818: 'complimentari',\n",
       " 819: 'stand',\n",
       " 820: 'numberamnumberpm',\n",
       " 821: 'load',\n",
       " 822: 'askd',\n",
       " 823: 'direct',\n",
       " 824: 'activ',\n",
       " 825: 'safe',\n",
       " 826: 'deal',\n",
       " 827: 'connect',\n",
       " 828: 'semest',\n",
       " 829: 'laptop',\n",
       " 830: 'swing',\n",
       " 831: 'normal',\n",
       " 832: 'via',\n",
       " 833: 'ago',\n",
       " 834: 'seen',\n",
       " 835: 'slow',\n",
       " 836: 'rental',\n",
       " 837: 'india',\n",
       " 838: 'rent',\n",
       " 839: 'ipod',\n",
       " 840: 'ladi',\n",
       " 841: 'through',\n",
       " 842: 'remind',\n",
       " 843: 'gym',\n",
       " 844: 'darren',\n",
       " 845: 'eye',\n",
       " 846: 'store',\n",
       " 847: 'ugh',\n",
       " 848: 'extra',\n",
       " 849: 'knew',\n",
       " 850: 'photo',\n",
       " 851: 'truth',\n",
       " 852: 'heard',\n",
       " 853: 'fill',\n",
       " 854: 'grin',\n",
       " 855: 'luck',\n",
       " 856: 'difficult',\n",
       " 857: 'john',\n",
       " 858: 'comp',\n",
       " 859: 'std',\n",
       " 860: 'usf',\n",
       " 861: 'request',\n",
       " 862: 'copi',\n",
       " 863: 'sunday',\n",
       " 864: 'link',\n",
       " 865: 'comin',\n",
       " 866: 'abiola',\n",
       " 867: 'dollarnumb',\n",
       " 868: 'rpli',\n",
       " 869: 'includ',\n",
       " 870: 'loan',\n",
       " 871: 'page',\n",
       " 872: 'txting',\n",
       " 873: 'sometim',\n",
       " 874: 'hmmm',\n",
       " 875: 'muz',\n",
       " 876: 'orchard',\n",
       " 877: 'kate',\n",
       " 878: 'ive',\n",
       " 879: 'rs',\n",
       " 880: 'regist',\n",
       " 881: 'bcoz',\n",
       " 882: 'teach',\n",
       " 883: 'road',\n",
       " 884: 'expect',\n",
       " 885: 'lover',\n",
       " 886: 'disturb',\n",
       " 887: 'wana',\n",
       " 888: 'somebodi',\n",
       " 889: 'rest',\n",
       " 890: 'small',\n",
       " 891: 'met',\n",
       " 892: 'discount',\n",
       " 893: 'monday',\n",
       " 894: 'fight',\n",
       " 895: 'silent',\n",
       " 896: 'warm',\n",
       " 897: 'door',\n",
       " 898: 'idea',\n",
       " 899: 'possibl',\n",
       " 900: 'fall',\n",
       " 901: 'whenev',\n",
       " 902: 'cancel',\n",
       " 903: 'fantasi',\n",
       " 904: 'fact',\n",
       " 905: 'slowli',\n",
       " 906: 'polic',\n",
       " 907: 'hr',\n",
       " 908: 'nah',\n",
       " 909: 'callertun',\n",
       " 910: 'wap',\n",
       " 911: 'england',\n",
       " 912: 'myself',\n",
       " 913: 'sick',\n",
       " 914: 'oop',\n",
       " 915: 'red',\n",
       " 916: 'situat',\n",
       " 917: 'short',\n",
       " 918: 'recent',\n",
       " 919: 'il',\n",
       " 920: 'repres',\n",
       " 921: 'gave',\n",
       " 922: 'men',\n",
       " 923: 'apart',\n",
       " 924: 'quot',\n",
       " 925: 'del',\n",
       " 926: 'soni',\n",
       " 927: 'lovabl',\n",
       " 928: 'pray',\n",
       " 929: 'wast',\n",
       " 930: 'trust',\n",
       " 931: 'sign',\n",
       " 932: 'kick',\n",
       " 933: 'admir',\n",
       " 934: 'deep',\n",
       " 935: 'hmv',\n",
       " 936: 'stupid',\n",
       " 937: 'sim',\n",
       " 938: 'somewher',\n",
       " 939: 'merri',\n",
       " 940: 'pete',\n",
       " 941: 'record',\n",
       " 942: 'immedi',\n",
       " 943: 'access',\n",
       " 944: 'custcar',\n",
       " 945: 'ex',\n",
       " 946: 'woke',\n",
       " 947: 'mm',\n",
       " 948: 'yep',\n",
       " 949: 'voic',\n",
       " 950: 'ldn',\n",
       " 951: 'style',\n",
       " 952: 'water',\n",
       " 953: 'opinion',\n",
       " 954: 'less',\n",
       " 955: 'member',\n",
       " 956: 'across',\n",
       " 957: 'cheap',\n",
       " 958: 'em',\n",
       " 959: 'ts',\n",
       " 960: 'ho',\n",
       " 961: 'gap',\n",
       " 962: 'fantast',\n",
       " 963: 'glad',\n",
       " 964: 'summer',\n",
       " 965: 'bag',\n",
       " 966: 'gettin',\n",
       " 967: 'wed',\n",
       " 968: 'poor',\n",
       " 969: 'asap',\n",
       " 970: 'otherwis',\n",
       " 971: 'ntt',\n",
       " 972: 'nyt',\n",
       " 973: 'convey',\n",
       " 974: 'regard',\n",
       " 975: 'p',\n",
       " 976: 'doctor',\n",
       " 977: 'energi',\n",
       " 978: 'write',\n",
       " 979: 'cover',\n",
       " 980: 'natur',\n",
       " 981: 'doin',\n",
       " 982: 'hw',\n",
       " 983: 'excus',\n",
       " 984: 'med',\n",
       " 985: 'empti',\n",
       " 986: 'bless',\n",
       " 987: 'serious',\n",
       " 988: 'mark',\n",
       " 989: 'forev',\n",
       " 990: 'password',\n",
       " 991: 'boss',\n",
       " 992: 'flight',\n",
       " 993: 'app',\n",
       " 994: 'sunshin',\n",
       " 995: 'lazi',\n",
       " 996: 'lect',\n",
       " 997: 'lift',\n",
       " 998: 'especi',\n",
       " 999: 'itself',\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El vocabulario es un diccionario con índices y palabras\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53ed8b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 4, 31, 170}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Devolvemos los índices de las palabras del vocabulario que aparecen en un email\n",
    "getIndices('email', vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5b20d",
   "metadata": {},
   "source": [
    "### Esto es para el entrenamiento de un modelo de Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2b24369",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(map(lambda x: getFeatureVector(getIndices(x, vocabulary), vocab_length), emails))\n",
    "X = pd.DataFrame(np.array(X).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2595b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17ca0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NativeBayes():\n",
    "    def __init__(self):\n",
    "        self.probabilidad_clase = dict()\n",
    "        self.probabilidad_palabra = dict()\n",
    "        self.clases = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Obtenemos las clases únicas, en este caso spam y ham\n",
    "        self.clases = np.unique(y)\n",
    "        \n",
    "        # Guardamos la probabilidad de cada clase y la probabilidad de cada palabra dado una clase en un diccionario para luego poder hacer las predicciones\n",
    "        for c in self.clases:\n",
    "            X_c = X[y == c]\n",
    "            self.probabilidad_clase[c] = X_c.shape[0] / X.shape[0]\n",
    "            \n",
    "            # Suavizado de Laplace: Se usa para evitar probabilidades de cero, ya que si predecimos un nuevo email que no contiene una palabra del vocabulario \n",
    "            # la probabilidad de esa palabra sería cero, lo cual afectaría el cálculo de la probabilidad total por lo que asumimos que todas las palabras\n",
    "            # al menos aparecen una vez en cada clase\n",
    "            self.probabilidad_palabra[c] = (X_c.sum(axis=0) + 1) / (X_c.shape[0] + 2)\n",
    "    \n",
    "    def predecir(self, X):\n",
    "        predicciones = []\n",
    "        \n",
    "        # Predecimos la clase para cada email\n",
    "        for i in range(X.shape[0]):\n",
    "            prediciones_por_clase = dict()\n",
    "            \n",
    "            # Probabilidad a priori de cada clase\n",
    "            for c in self.clases:\n",
    "                # Vamos a usar logaritmos para evitar underflow y para que las probabilidades sean más manejables, tambien para evitar\n",
    "                # multiplicar y mejor sumar, log(a x b) = log(a) + log(b)\n",
    "                probabilidad_clase_c = np.log(self.probabilidad_clase[c])\n",
    "                \n",
    "                # Tomamos en cuenta la probabilidad de cada palabra en el email dado la clase, pero también la probabilidad de que no aparezca\n",
    "                # ya que ambas cosas nos dan información sobre la clase del email\n",
    "                probabilidad = X.iloc[i] * np.log(self.probabilidad_palabra[c]) + (1 - X.iloc[i]) * np.log(1 - self.probabilidad_palabra[c])\n",
    "                probabilidad = probabilidad.sum()\n",
    "                probabilidad_final = probabilidad_clase_c + probabilidad\n",
    "                prediciones_por_clase[c] = probabilidad_final\n",
    "            \n",
    "            # Tomamos la clase con la probabilidad más alta, es decir si P(clase|email) es mayor para spam que para ham, entonces el email es spam\n",
    "            predicciones.append(max(prediciones_por_clase, key=prediciones_por_clase.get))\n",
    "        \n",
    "        return np.array(predicciones)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Retorna las probabilidades REALES de cada clase para cada email\n",
    "        Usa la fórmula completa: P(clase|email) = P(email|clase) × P(clase) / P(email)\n",
    "        \"\"\"\n",
    "        probabilidades = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Calculamos P(email|clase) × P(clase) para cada clase\n",
    "            numeradores = dict()\n",
    "            \n",
    "            for c in self.clases:\n",
    "                # P(clase) - probabilidad a priori\n",
    "                #Es la proporción de correos spam o ham en el dataset.\n",
    "                #La usamos porque Bayes dice que cada clase tiene un peso inicial antes de ver el email.\n",
    "                prior = self.probabilidad_clase[c]\n",
    "                \n",
    "                # P(email|clase) - likelihood\n",
    "                # Si la palabra aparece en el email entonces se va a multiplicar P(palabra|clase)\n",
    "                # Si la palabra no aparece en el email se multiplica por (1 - P(palabra|clase))\n",
    "                likelihood = 1.0\n",
    "                for j in range(len(X.iloc[i])):\n",
    "                    if X.iloc[i].iloc[j] == 1:  # palabra presente\n",
    "                        likelihood *= self.probabilidad_palabra[c].iloc[j]\n",
    "                    else:  # palabra ausente\n",
    "                        likelihood *= (1 - self.probabilidad_palabra[c].iloc[j])\n",
    "                \n",
    "                # P(email|clase) × P(clase)\n",
    "                #Es la probabilidad conjunta de que el email y la clase ocurran juntos.\n",
    "                numeradores[c] = likelihood * prior\n",
    "            \n",
    "            # P(email) = suma de todos los numeradores\n",
    "            #Es la probabilidad total del email sin importar la clase.\n",
    "            #Se necesita para normalizar y que las probabilidades finales sumen 1.\n",
    "            denominador = sum(numeradores.values())\n",
    "            \n",
    "            # P(clase|email) = P(email|clase) × P(clase) / P(email)\n",
    "            # Esto nos da la probabilidad de que el email pertenezca a cada clase (Spam o Ham)\n",
    "            probabilidades_reales = [numeradores[c] / denominador for c in sorted(self.clases)]\n",
    "            probabilidades.append(probabilidades_reales)\n",
    "            \n",
    "        return np.array(probabilidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ea4d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones de clase: ['ham' 'ham' 'spam' 'ham' 'spam']\n",
      "Probabilidades [P(ham), P(spam)]:\n",
      "Email 0: P(ham)=0.9991, P(spam)=0.0009\n",
      "Email 1: P(ham)=0.9994, P(spam)=0.0006\n",
      "Email 2: P(ham)=0.0000, P(spam)=1.0000\n",
      "Email 3: P(ham)=1.0000, P(spam)=0.0000\n",
      "Email 4: P(ham)=0.0000, P(spam)=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3245     ham\n",
       "944      ham\n",
       "1044    spam\n",
       "2484     ham\n",
       "812     spam\n",
       "Name: v1, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de uso de predict_proba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo = NativeBayes()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones de clase\n",
    "predicciones = modelo.predecir(X_test.head(5))\n",
    "print(\"Predicciones de clase:\", predicciones)\n",
    "\n",
    "# Probabilidades\n",
    "probabilidades = modelo.predict_proba(X_test.head(5))\n",
    "print(\"Probabilidades [P(ham), P(spam)]:\")\n",
    "for i, probs in enumerate(probabilidades):\n",
    "    print(f\"Email {i}: P(ham)={probs[0]:.4f}, P(spam)={probs[1]:.4f}\")\n",
    "\n",
    "y_test.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
